{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This excercise explains core components of the langchain, will demo the following concepts:\n",
    "\n",
    "# - LLM Models and model wrappers (here we use open AI LLM)\n",
    "# - Prompt templates (which are used to generate prompts for the models)\n",
    "# - Chains (which get input from one chain to another and helps in chaining multipls repsponses and represent it in a more meaningful response)\n",
    "# - Embeddings and vector stores\n",
    "# - Agents\n",
    "\n",
    "# reference: https://www.youtube.com/watch?v=aywZrzNaKjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) # This is to load your .env file with your credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LLM Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of how to use the OpenAI API with the langchain library\n",
    "# you can install langchain with pip install langchain\n",
    "# and you can install OpenAI with pip install openai\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is an example Python script that trains a simple neural network on simulated data using the popular library TensorFlow:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.random.randint(0, 2, 1000)\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Dense(32, activation='relu', input_shape=(2,)),\n",
      "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "In this script, we first generate simulated data with 2 features and binary labels. We then define a simple neural network with one hidden layer and an output layer. We compile the model with binary cross-entropy loss and train it on the simulated data for 10 epochs. Finally, we evaluate the model on the training data and print the loss and accuracy.\n",
      "\n",
      "You can run this script in a Python environment with TensorFlow installed to train a neural network on simulated data.\n"
     ]
    }
   ],
   "source": [
    "# this is an example of how to use the ChatOpenAI class with the langchain library\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data\"),\n",
    "]\n",
    "response = chat(messages)\n",
    "print(response.content, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of how to use the PromptTemplate class with the langchain library\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in couple of lines.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\"\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in couple of lines.\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient descent is an optimization algorithm used to minimize the error of a model by adjusting its parameters iteratively. It works by calculating the gradient of the loss function with respect to each parameter and moving in the direction opposite to the gradient to reach the optimal set of parameters that minimize the error.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call llm with the prompt\n",
    "llm(prompt.format(concept=\"gradient descent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An autoencoder is a type of neural network that learns to encode input data into a lower-dimensional representation and reconstruct the original input from this encoded representation. It is commonly used for dimensionality reduction, data compression, and anomaly detection tasks in machine learning.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt) # LLMChain is a class that chains multiple LLMs together\n",
    "\n",
    "# Run the chain only specifying the input variables\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"machine_learning_concept\"],\n",
    "    template=\"Turn the concept description of {machine_learning_concept} and explain it to me in layman's terms.\")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of neural network that learns to encode input data into a lower dimensional representation, known as the \"code\" or \"latent space\", before decoding it back into the original input data. This unsupervised learning technique is often used for tasks such as dimensionality reduction, anomaly detection, and data denoising.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mAn autoencoder is like a translator that takes a complex piece of information, like a paragraph, and simplifies it into a shorter version, known as a code. It then translates this code back into the original paragraph. This can be helpful for organizing data, finding errors, or cleaning up messy information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "An autoencoder is like a translator that takes a complex piece of information, like a paragraph, and simplifies it into a shorter version, known as a code. It then translates this code back into the original paragraph. This can be helpful for organizing data, finding errors, or cleaning up messy information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "total_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the total chain with the input \"autoencoder\" and print the resulting explanation.\n",
    "explanation = total_chain.run(\"autoencoder\")\n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embeddings and Vector stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap= 0)\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='An autoencoder is like a translator that takes a complex piece of information, like a paragraph, and', metadata={}),\n",
       " Document(page_content='simplifies it into a shorter version, known as a code. It then translates this code back into the', metadata={}),\n",
       " Document(page_content='original paragraph. This can be helpful for organizing data, finding errors, or cleaning up messy', metadata={}),\n",
       " Document(page_content='information.', metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autoencoder is like a translator that takes a complex piece of information, like a paragraph, and'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2127,\n",
       " 3313,\n",
       " 28106,\n",
       " 374,\n",
       " 1093,\n",
       " 264,\n",
       " 46588,\n",
       " 430,\n",
       " 5097,\n",
       " 264,\n",
       " 6485,\n",
       " 6710,\n",
       " 315,\n",
       " 2038,\n",
       " 11,\n",
       " 1093,\n",
       " 264,\n",
       " 14646,\n",
       " 11,\n",
       " 323]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "embeddings = tiktoken.get_encoding(\"cl100k_base\")\n",
    "query_results = embeddings.encode(texts[0].page_content)\n",
    "# query_results = embeddings.embed_query(texts[0].page_content)\n",
    "query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-demo\"\n",
    "\n",
    "# Check if the index already exists, if not, create it\n",
    "if index_name not in pinecone.list_indexes():\n",
    "\tpinecone.create_index(index_name, dimension=len(embeddings.encode(texts[0].page_content)))\n",
    "\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"explain autoencoder\"\n",
    "results = search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rrajeshrajappan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\llms\\openai.py:170: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "C:\\Users\\rrajeshrajappan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain\\llms\\openai.py:624: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe code provided will print \"Hello, World!\" to the console.\n",
      "Action: Python REPL\n",
      "Action Input: print('Hello, World!')\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHello, World!\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe code executed successfully and printed the expected output.\n",
      "Final Answer: Hello, World!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.run(\"print('Hello, World!')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
